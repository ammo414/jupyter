{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# File Change Semantics\n",
    "\n",
    "#### Anmol Eswarapu\n",
    "\n",
    "Heim introduced the concept of file change semantics, a revival and modification of an old approach to thinking about indeterminates and determinates in multi-sentence discourses. In this project, I will be attempting to implement a file change semantic parser to parse a few simple sentences. I will explain the strengths and shortcomings of my alogrithm and discuss where it can be improved.\n",
    "\n",
    "This project will be divided into creating a context free grammar that can generate the sentences I wish to explore (but also overgenerates), a generator which will generate grammatically correct strings and syntax trees given our CFG, and our semantic parser."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "1. [What is File Change Semantics?](#What-is-File-Change-Semantics?)\n",
    "2. [CFG and Generator](#CFG-and-Generator)\n",
    "    1. [CFG](#CFG)\n",
    "    2. [Generator](#Generator)\n",
    "3. [Syntax Tree Functions](#Syntax-Tree-Functions)\n",
    "4. [Exploring Generated Strings](#Exploring-Generated-Strings)\n",
    "5. [Semantic Parser](#Semantic-Parser)\n",
    "    1. [Initializing](#Initializing)\n",
    "    2. [Updating](#Updating)\n",
    "    3. [Definite Articles](#Definite-Articles)\n",
    "    4. [Final Algorithm](#Final-Algorithm)\n",
    "6. [Testing](#Testing)\n",
    "7. [Summary and Thoughts](#Summary-and-Thoughts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is File Change Semantics?\n",
    "\n",
    "Heim's File Change Semantics is an approach to understanding the difference between indefinite articles and definite articles. Heim argues that when a new object is introduced into a discourse, it should be introduced with an indefinite. On the other hand, an object is paired with a definite only when that object is already in the discourse - either it has already been introduced in a prior phrase, or it is part of the common knowledge. (For example, `a woman looked at the sun` is perfectly meaningful even though we never introduced `the sun` in this discourse because we already know what `the sun` is.)\n",
    "\n",
    "Consider $(i)$:\n",
    "\n",
    "\n",
    "\n",
    "    a) A woman was comforted by a dog. b) The woman pet the dog. c) The dog jumped over a fence.\n",
    "    \n",
    "$ \\tag i $\n",
    "\n",
    "According to Heim, we want to have a file, call it $F_0$, that starts off empty. \n",
    "\n",
    "|$F_0$|\n",
    "|-|\n",
    "\n",
    "After a) is uttered, two cards are put into the file, each labelled with a number, call them $0$ and $1$. We want the file cards to look as follows:\n",
    "\n",
    "\n",
    "|$F_1$|$0$| $1$|\n",
    "|-|--| --|\n",
    "||- woman|- dog|\n",
    "||- was comforted by $1$|- comforted $0$|\n",
    "\n",
    "Each card is supposed to represent a discourse referrent (for more information, read Heim's 1983 paper).\n",
    "\n",
    "So $F_0$ is updated, now call it $F_1$\n",
    "\n",
    "After b) is uttered, $0$ and $1$ are updated; no new cards are added.\n",
    "\n",
    "|$F_2$|$0$| $1$|\n",
    "|-|-| --|\n",
    "||- woman|- dog|\n",
    "    ||- was comforted by $1$|- comforted $0$|\n",
    "||- pet $1$|- was pet by $0$|\n",
    "\n",
    "Finally, after c) is uttered, a new card is added, call it $2$. Card $1$ is also updated as follows:\n",
    "\n",
    "|$F_3$|$0$| $1$|$2$\n",
    "|-|-| --|-|\n",
    "||- woman|- dog|- fence|\n",
    "||- was licked by $1$|- comforted $0$|- $1$ jumped over|\n",
    "||- pet $1$|- was pet by $0$||\n",
    "|||- jumped over $1$||\n",
    "\n",
    "This file represents all the semantic knowledge contained in $(i)$. The basic algorithm is straightfoward and the knowldge base can easily be updated, both when it comes to new objects as well as new details about previous objects.\n",
    "\n",
    "My task is to implement Heim's procedure in python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CFG and Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CFG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I modify a few sentences from Heim (1983):\n",
    "\n",
    "'A woman was comforted by a dog. The woman pet the dog. The dog jumped over a fence.' $\\tag i$\n",
    "\n",
    "\n",
    "\n",
    "I've modified these sentences from their originals to remove pronouns (`it` $\\rightarrow$ `the dog`, and `she` $\\rightarrow$ `the woman` ) and also to make the subject matter less violent. I remove pronouns because the focus of this project is anaphoric determinates, and pronouns add a layer of complexity that is not needed for our focus. I make the subject matter less violent because I generally don't like violence. \n",
    "\n",
    "A CFG for this discourse is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = {\n",
    "    'S'     : [('NP', 'VP')],                                             ## S  --> NP VP\n",
    "    'NP'    : [('INDEF', 'N'), ('DEF', 'N')],                             ## NP --> INDEF N | DEF N           \n",
    "    'VP'    : [('V', 'NP'), ('V', 'PP'), ('Vaux', 'Vbar')],               ## VP --> V NP    | Vaux VPP\n",
    "    'Vbar'  : [('V', 'PP')],                                              ## etc.\n",
    "    'PP'    : [('PREP', 'NP')],\n",
    "    \n",
    "    'INDEF' : [('a',)],\n",
    "    'DEF'   : [('the',)],\n",
    "    'N'     : [('woman',), ('dog',), ('fence',)],\n",
    "    'Vaux'  : [('was',)],\n",
    "    'V'     : [('comforted',), ('pet',), ('jumped',)],\n",
    "    'PREP'  : [('by',), ('over',)],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator\n",
    "\n",
    "This is a modification of the CFG generator that we created in class. Rather than outputting a string, this function creates a syntax tree, which allows for easier analysis later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## syntax tree class\n",
    "\n",
    "class Tree(object):\n",
    "    def __init__(self, object):\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "        self.data = object\n",
    "        self.head = None\n",
    "        \n",
    "## initialize root of tree\n",
    "\n",
    "test = Tree('S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## generator function\n",
    "\n",
    "def gen(gram, ptr):\n",
    "    \n",
    "    \n",
    "    branches = random.choice(gram[ptr.data])\n",
    "    \n",
    "    #print(branches[1], 'branches')\n",
    "                \n",
    "    for val, branch in enumerate(branches): ##for each branch of RHS,\n",
    "        if val == 0:\n",
    "            ptr.left = Tree(branch) ## add to tree\n",
    "            ptr.left.head = ptr\n",
    "            \n",
    "        if val == 1:\n",
    "            ptr.right = Tree(branch)\n",
    "            ptr.right.head = ptr\n",
    "            \n",
    "        if branch in gram: ## if tree can be continued\n",
    "            if val == 0:\n",
    "                gen(gram, ptr.left)\n",
    "            else:\n",
    "                gen(gram, ptr.right)\n",
    "                \n",
    "    return ptr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.Tree at 0x7f1f1063d1d0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen(cfg, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Syntax Tree Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a syntax tree, we want functions to read and traverse this tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## print end nodes\n",
    "\n",
    "def LeafNodes(ptr):\n",
    "    res = []\n",
    "    if ptr == None: ## if node has no data\n",
    "        return\n",
    "    \n",
    "    if ptr.left == None and ptr.right == None: ## if node has no children\n",
    "        res.append(ptr.data)\n",
    "        return res\n",
    "    \n",
    "    if ptr.left != None:\n",
    "        res = res + LeafNodes(ptr.left)\n",
    "    if ptr.right != None:\n",
    "        res = res + LeafNodes(ptr.right)\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', 'fence', 'was', 'jumped', 'over', 'the', 'fence']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LeafNodes(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## return a list of subtrees specified by POS\n",
    "\n",
    "def findNode(ptr, pos):\n",
    "    res = []\n",
    "    if ptr != None:\n",
    "        if ptr.data is pos:\n",
    "            res.append(ptr)\n",
    "        res = res + findNode(ptr.left, pos)\n",
    "        res = res + findNode(ptr.right, pos)\n",
    "        \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'fence']\n",
      "['the', 'fence']\n",
      "\n",
      "['was', 'jumped', 'over', 'the', 'fence']\n"
     ]
    }
   ],
   "source": [
    "for x in findNode(test, 'NP'):\n",
    "    print(LeafNodes(x))\n",
    "    \n",
    "print('')\n",
    "for x in  findNode(test, 'VP'):\n",
    "    print(LeafNodes(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring Generated Strings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to file change semantics, the first time a new object is introduced to the discourse, it must be accompanied with an indeterminate article. As can be seen by some of the sample sentences below, this is not always the case with our CFG. We can say that this grammar overgenerates: though it does generate all the strings we are interested in, e.g. $(i)$ above, it is also able to generate strings that we deem ungrammatical for our purposes. Nonetheless, the CFG is sufficient; we just have to be selective of which sentences we choose when testing our program. \n",
    "\n",
    "\n",
    "It's also interesting to note that while some of these sentences might seem awkward at first glance, they are actually all gramatically correct in English. For instance, below we have the sentence `the fence comforted by a fence`. While this is unwieldly, it can make sense: the fence comforted something, and it did so by a fence. All other sentences generated with this CFG should be similarly sensible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the dog was jumped over a dog.\n",
      "\n",
      "the fence comforted over a dog.\n",
      "\n",
      "a woman was jumped over the woman.\n",
      "\n",
      "a woman comforted the dog.\n",
      "\n",
      "a fence was pet by a fence.\n",
      "\n",
      "the woman pet over a fence.\n",
      "\n",
      "the fence jumped over a woman.\n",
      "\n",
      "the woman was pet over the dog.\n",
      "\n",
      "a woman jumped by the woman.\n",
      "\n",
      "a fence jumped by a woman.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "random.seed( 1502 )\n",
    "\n",
    "for x in range(10):\n",
    "    gen(cfg, test)\n",
    "    print(' '.join(LeafNodes(test))+'.\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic Parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing\n",
    "\n",
    "The first step in  the FCS procedure is identifying any indefinite Noun Phrases and adding the corresponding Noun to the file. Like Heim, I will label each card with a number. I'll be using Python's dictionary data structure to represent the file, with the key/value pairs being card label/card contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'fence', 'jumped', 'by', 'a', 'woman']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## print a test sentence\n",
    "LeafNodes(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize(ptr, file):\n",
    "    m = len(file)\n",
    "    \n",
    "    n = len(findNode(ptr, 'INDEF')) ## number of INDEF NP\n",
    "    for i, val in enumerate(findNode(ptr, 'INDEF')):\n",
    "        x = val.head.right.left.data ##associated noun\n",
    "        if x not in file:\n",
    "            file[m+i] = [x] ##add card to file\n",
    "    return file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 \n",
      " ['fence'] \n",
      "\n",
      "1 \n",
      " ['woman'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "file1 = {}\n",
    "file1 = initialize(test, file1)\n",
    "\n",
    "for x in file1:\n",
    "    print( x, '\\n', file1[x], '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updating\n",
    "\n",
    "The next step in the procedure is identifying any descriptive phrases associated with each noun, and updating the file cards appropriately. Since the verbs in our model are all transitive, there are two ways a card can be updated: the noun in question can be either the subject or the object of the sentence, and so the update algorithm has to be appropriately specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateObject(ptr, file, noun):\n",
    "    desc = LeafNodes(ptr)[:-2] ## description is S - last NP\n",
    "    \n",
    "    for x in file:\n",
    "        for y in range(len(desc)-1):\n",
    "            if file[x][0] == desc[y]:\n",
    "                desc[y] = str(x) ## replace noun with card label\n",
    "                desc.pop(y-1)\n",
    "                \n",
    "    for x in file:\n",
    "        if file[x][0] == noun:\n",
    "            file[x].append(' '.join(desc))\n",
    "    return file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateSubject(ptr, file, noun):\n",
    "    desc = LeafNodes(ptr)[2:] ## description is S - first NP\n",
    "    \n",
    "    for x in file:\n",
    "        for y in range(len(desc)):\n",
    "            if file[x][0] == desc[y]:\n",
    "                desc[y] = str(x)\n",
    "                desc.pop(y-1)\n",
    "                \n",
    "    for x in file:\n",
    "        if file[x][0] == noun:\n",
    "            file[x].append(' '.join(desc))\n",
    "    return file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(ptr, file):\n",
    "    for x in file:\n",
    "        noun = file[x][0]\n",
    "        if findNode(ptr, noun):\n",
    "            \n",
    "            if findNode(ptr, noun)[0].head.head.head.data == 'S':\n",
    "                updateSubject(ptr, file, noun)\n",
    "            else:\n",
    "                updateObject(ptr, file, noun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'fence', 'jumped', 'by', 'a', 'woman']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## recall test sentence\n",
    "LeafNodes(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 \n",
      " ['fence', 'jumped by 1'] \n",
      "\n",
      "1 \n",
      " ['woman', '0 jumped by'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "update(test, file1)\n",
    "for x in file1:\n",
    "    print(x, '\\n', file1[x], '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definite Articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next part of the process is dealing with definite articles. If a noun phrase has a definite article, then the noun w as already introduced. Thus, no card has to be initialized, but updating is identical from before alebit with a new sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'fence', 'jumped', 'by', 'a', 'woman']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LeafNodes(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'dog', 'was', 'pet', 'over', 'the', 'fence']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.seed(217)\n",
    "\n",
    "test2 = Tree('S')\n",
    "gen(cfg, test2)\n",
    "LeafNodes(test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "initialize(test2, file1)\n",
    "update(test2, file1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 \n",
      " ['fence', 'jumped by 1', '2 was pet over'] \n",
      "\n",
      "1 \n",
      " ['woman', '0 jumped by'] \n",
      "\n",
      "2 \n",
      " ['dog', 'was pet over 0'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for x in file1:\n",
    "    print(x, '\\n', file1[x], '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Algorithm\n",
    "\n",
    "Putting together our various finctions, we end up with a small, tody program capable of file change semantic parsing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filechangesemantics(discourse, file):\n",
    "    \n",
    "    # discourse : a list of sentence tree roots\n",
    "    # file : a dict to store card info (key/val paries of card label/card info)\n",
    "    \n",
    "    for sentence in discourse:\n",
    "        initialize(sentence, file)\n",
    "        update(sentence, file)\n",
    "    return file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prettyprint(file):\n",
    "    # prints filecards in a pretty way\n",
    "    \n",
    "    for x in file:\n",
    "        print(x, '\\n', file[x], '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "\n",
    "Generating three grammatical sentences, we'll evaluate what our algorithm outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "root1, root2, root3 = Tree('S'), Tree('S'), Tree('S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'fence', 'pet', 'a', 'dog']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.seed(6410)\n",
    "\n",
    "gen(cfg, root1)\n",
    "LeafNodes(root1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', 'dog', 'pet', 'by', 'a', 'woman']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.seed(7959)\n",
    "\n",
    "gen(cfg, root2)\n",
    "LeafNodes(root2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', 'woman', 'pet', 'over', 'the', 'fence']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.seed(1299)\n",
    "\n",
    "gen(cfg, root3)\n",
    "LeafNodes(root3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 \n",
      " ['fence', 'pet 1', '2 pet over'] \n",
      "\n",
      "1 \n",
      " ['dog', '0 pet', 'pet by 2'] \n",
      "\n",
      "2 \n",
      " ['woman', '1 pet by', 'pet over 0'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "discourse1 = [root1, root2, root3]\n",
    "\n",
    "file = {}\n",
    "file = filechangesemantics(discourse1, file)\n",
    "prettyprint(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Thoughts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Efficiency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm runs in `O(nm)` time at worst, and `O(n)` at best, where `n` =  total number of words in all sentences, and `m` = number of determinate `NP`s. Each time a determinate `NP` is found, the parser has to look through its file to see if that card already exists. This is an incredibly efficient algorithm, which makes it usable for both real world applications, and a candidate for explaining what's going on in our heads."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Edge Cases\n",
    "While its great to see the algorithm work on proper inputs, it's interesting to see what the algorithm does with improper inputs, e.g. objects paired with determinates but not yet properly introduced in the discourse:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', 'fence', 'was', 'comforted', 'by', 'the', 'dog']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.seed(198)\n",
    "\n",
    "root = Tree('S')\n",
    "gen(cfg, root)\n",
    "\n",
    "LeafNodes(root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "prettyprint(filechangesemantics([root], {}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function outpupts nothing, which is exactly what we want; since we have no card already in place for fence or dog, there is no card to update. and so we see no files being initialized or updated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', 'fence', 'jumped', 'over', 'a', 'dog']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.seed(773)\n",
    "\n",
    "root = Tree('S')\n",
    "\n",
    "gen(cfg, root)\n",
    "LeafNodes(root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 \n",
      " ['dog', 'the fence jumped over'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "prettyprint(filechangesemantics([root], {}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time, the dog was able to be initialized, while the fence wasn't. Again, exactly what we would expect.\n",
    "\n",
    "What's interesting about these failures is that we can see that our semantic parser is actually able to differentiate between indefinites and definites; the parser does not look at the syntax, but purely the article type when it chooses whether to initialize another object. Some of the grammaticality resides in the semantics of the discourse, rather than the syntax of the sentences. After all, `The fence jumped over a dog` would be considered ungrammatical as the first sentence in a discourse, but might be grammatical if its the second or third.\n",
    "\n",
    "As mentioned before, sentences like `A woman looked at the sun` seem to be problematic. However, \"common knowledge\" objects like `the sun` are actually easily accounted for; rather than starting with an entirely empty file, we simply need a starting file with cards already in place for common knowledge objects - `the sun`, `the president`, etc. Even sentences like `A man ran to the store`, where `the store` is actually an arbitrary store, rather than a particular one, can make sense with a sophisticated enough starting file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'fence', 'comforted', 'over', 'a', 'fence']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.seed(55)\n",
    "\n",
    "root = Tree('S')\n",
    "gen(cfg, root)\n",
    "LeafNodes(root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'dog', 'was', 'jumped', 'by', 'the', 'fence']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.seed(299)\n",
    "\n",
    "root2 = Tree('S')\n",
    "gen(cfg, root2)\n",
    "LeafNodes(root2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 \n",
      " ['fence', 'comforted over 0', 'comforted over 0', '2 was jumped by', '2 was jumped by'] \n",
      "\n",
      "1 \n",
      " ['fence', 'comforted over 0', 'comforted over 0', '2 was jumped by', '2 was jumped by'] \n",
      "\n",
      "2 \n",
      " ['dog', 'was jumped by 0'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "prettyprint(filechangesemantics([root, root2], {}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentences like `a fence comforted over a fence` are problematic, and our algorithm cannot deal with it properly, as can be seen here. Since `fence` is introduced to the discourse twice, two `fence` cards are initialized. However, there's no way to tell which card should be updated when new information is introduced. Nor can we tell which fence should be mentioned in other cards. But this makes sense, and is analogous to how we, as humans, would deal with such ambiguity in normal conversation. If the sentences above were uttered to us, we would only be confused and unable to properly process the semantic information. We might very well also create two `fence` cards, and update both of them when new information about either one is brought up. Or we might do nothing at all. While our algorithm doesn't deal with it the best way possible, it still does an admirable job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Research\n",
    "\n",
    "Singular Pronouns are somewhat easy to implement; treating them as determinate `NP`s works to an extent, though dealing with the anaphora is troublesome. If the parser ever sees a `she`, for example, we would have to tell it to update a card belonging to a `woman`, or a `girl`, or a `mom`, etc. However, the ambiguity that might arise from such a procedure would be hard to deal with in many cases.\n",
    "\n",
    "\n",
    "While the parser itself is sophisticated enough, our CFG isn't. Since some of the grammaticality lies in the semantics, rather than purely in the syntax, this isn't surprising. Nonetheless, we could've created multiple CFGs, one for the first sentence in a discourse, and one for other sentences. Doing so would have made it easier to create proper discourses to test our parser.\n",
    "\n",
    "This program on the whole is a successful implementation. It can parse the first few sentences that Heim offers in her paper. However, a lot of work can still be done. Other than the suggestions I've already offered here, dealing with more complex grammar, such as relative clauses or even just adjective phrases, would be good places to continue researching."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
